# Claude Memory System - Environment Configuration
# Copy this file to .env and fill in your values
#
# SETUP INSTRUCTIONS:
# 1. Copy this file: cp .env.example .env
# 2. Update CLAUDE_WORKSPACE_ROOT to your workspace path
# 3. Generate a secure password for CONTEXT_DB_PASSWORD
# 4. Optionally customize ports and models
# 5. Never commit .env to git (it's in .gitignore)

# ============================================================
# WORKSPACE CONFIGURATION
# ============================================================

# Root directory where Claude Code accesses your workspace
# This path will be mounted as /claude-workspace in containers (read-only)
#
# Examples:
#   macOS:   /Users/yourname/workspace
#   Linux:   /home/yourname/projects
#   Windows: /mnt/c/Users/yourname/Projects (WSL path)
#
# REQUIRED: Update this to your actual workspace root
CLAUDE_WORKSPACE_ROOT=/Users/yourname/workspace

# ============================================================
# DATABASE CONFIGURATION
# ============================================================

# Database name (default: claude_memory)
POSTGRES_DB=claude_memory

# Database user (default: memory_admin)
POSTGRES_USER=memory_admin

# Database password
# SECURITY: Generate a strong random password
# Example: openssl rand -base64 32
# REQUIRED: Change this before deployment!
CONTEXT_DB_PASSWORD=your_secure_password_here

# PostgreSQL host port (default: 5435)
# Change if port 5435 is already in use on your system
POSTGRES_HOST_PORT=5435

# ============================================================
# PROCESSOR SERVICE CONFIGURATION
# ============================================================

# Processor API port (default: 3200)
# Change if port 3200 is already in use on your system
PROCESSOR_HOST_PORT=3200

# Embedding model (sentence-transformers)
# Default: sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)
# Alternatives: sentence-transformers/all-mpnet-base-v2 (768 dimensions)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Summary model (Ollama)
# Default: llama3.2:latest
# Alternatives: llama3:8b, mistral:latest, phi3:latest
SUMMARY_MODEL=llama3.2:latest

# Enable real embeddings (true/false)
# Set to "false" for testing without embeddings
USE_REAL_EMBEDDINGS=true

# Enable AI summaries (true/false)
# Set to "false" for testing without Ollama
USE_AI_SUMMARIES=true

# Logging level (debug, info, warn, error)
LOG_LEVEL=info

# ============================================================
# OLLAMA CONFIGURATION
# ============================================================

# Ollama host port (default: 11434)
# Change if port 11434 is already in use on your system
OLLAMA_HOST_PORT=11434

# ============================================================
# MCP SERVER CONFIGURATION (Optional - Phase 6B)
# ============================================================

# MCP Server host port (default: 9001)
# Uncomment when MCP server is added to docker-compose
# MCP_SERVER_HOST_PORT=9001

# ============================================================
# ADVANCED CONFIGURATION
# ============================================================

# PostgreSQL shared buffers (default: 256MB)
# Increase for better performance on systems with more RAM
POSTGRES_SHARED_BUFFERS=256MB

# PostgreSQL max connections (default: 100)
POSTGRES_MAX_CONNECTIONS=100

# ============================================================
# NOTES
# ============================================================

# Port Allocations Summary:
# - ${POSTGRES_HOST_PORT} (5435): PostgreSQL + pgvector
# - ${PROCESSOR_HOST_PORT} (3200): Context Processor API
# - ${OLLAMA_HOST_PORT} (11434): Ollama AI
# - ${MCP_SERVER_HOST_PORT} (9001): MCP Server (Phase 6B)

# Docker Volume Mounts:
# - ${CLAUDE_WORKSPACE_ROOT} → /claude-workspace (read-only)
# - ./processor/logs → /app/logs (processor logs)
# - claude-memory-db-data → /var/lib/postgresql/data (database persistence)
# - claude-memory-ollama → /root/.ollama (Ollama models)

# Architecture:
# - All processing happens out-of-band (zero context consumption)
# - Database stores conversation snapshots with vector embeddings
# - Semantic search via pgvector (cosine similarity)
# - AI summaries via Ollama (runs locally)
